{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236c1f08-3812-4e7f-8f96-02bb9b45cf39",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f1ed4-62b8-4f78-ac52-d2ce4b554530",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Data Loading and Augmentation Using Keras</font></h1>\n",
    "\n",
    "<h1 align=left><font size = 5>Building efficient data loaders with Keras</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08307252-062f-4551-9d39-893b55794f4f",
   "metadata": {},
   "source": [
    "**Estimated time:** 30 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7876ae1d-8869-4c4b-8bfa-8967405ab25f",
   "metadata": {},
   "source": [
    "<h2>Learning objective</h2>\n",
    "After completing this lab, you'll be able to:\n",
    "\n",
    "- Build and test a custom data generator in Keras for efficient, on-the-fly loading, and preprocessing of image datasets.\n",
    "- Use Keras’s `image_dataset_from_directory` utility to load and preprocess datasets with optimized performance using the `tf.data` API.\n",
    "- Apply image augmentation and performance optimizations such as `.map()`, `.cache()`, and `.prefetch()` to build high-throughput data pipelines.\n",
    "- Compare custom and built-in Keras data loaders in terms of code complexity, flexibility, and runtime efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358e2458-5a18-4822-b65f-26e42be8c071",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you'll use two **Keras** functions to build efficient, automated data pipelines: custom data generator and the in-built Keras utility.\n",
    "\n",
    "1.  **Custom data generator:** You'll write a Python generator function to load, preprocess, and yield batches of data on-the-fly. This will help you understand the working of a production-level data generator.\n",
    "2.  **In-built Keras utility:** You will use the highly optimized `tf.keras.utils.image_dataset_from_directory` function, which is the standard, production-ready approach for most image classification tasks.\n",
    "\n",
    "Finally, you will compare these two methods to understand their trade-offs in terms of simplicity, features, and performance.\n",
    "\n",
    "**Note:** You’ll complete hands-on tasks embedded throughout the lab. These tasks are strategically designed to reinforce key concepts and deepen your understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d348fca-b62a-4ad3-8775-f93758015ef1",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. [Configure the setup](#Configure-the-setup)\n",
    "3. [What is custom data generator?](#What-is-custom-data-generator?)\n",
    "4. [What is Keras' built-in utility?](#What-is-Keras'-built-in-utility?)\n",
    "5. [Comparison and analysis](#Comparison-and-analysis)\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe99d3c8-5961-4d9a-bf2a-ce9185a36d15",
   "metadata": {},
   "source": [
    "### Install required libraries\n",
    "\n",
    "The following required libraries are not preinstalled in the Skills Network Labs environment. __You must run the following cell__ to install them. The installation might take a few minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0d144-3dce-48cd-841a-821ce954671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "%%time\n",
    "%pip install numpy==1.26 matplotlib==3.9.2 tensorflow==2.19\n",
    "%pip install torch==2.8.0+cpu torchvision==0.23.0+cpu torchaudio==2.8.0+cpu \\\n",
    "--index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fe1662-4dc5-4381-a391-d7b3bc69f1df",
   "metadata": {},
   "source": [
    "#### Check whether the libraries were installed successfully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c4892-0e82-4639-b516-5b5ba90c5b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = captured_output.stdout\n",
    "lines = output_text.splitlines()\n",
    "output_last_10_lines = '\\n'.join(lines[-10:])\n",
    "if \"error\" in output_last_10_lines.lower():\n",
    "    print(\"Library installation failed!\")\n",
    "    print(\"--- Error Details ---\")\n",
    "    print(output_last_10_lines)\n",
    "else:\n",
    "    print(\"Library installation was successful, let's proceed ahead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf3224-63d7-4edb-9345-e3650b209a51",
   "metadata": {},
   "source": [
    "## Configure the setup\n",
    "\n",
    "First, install and import the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df7550-ac56-40f7-a032-f78c5808e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import skillsnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b468b-edf1-482f-be00-d482c712a719",
   "metadata": {},
   "source": [
    "## Download data\n",
    " Download and extract data from the cloud.\n",
    " \n",
    " Define the data directories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196511c1-6ae7-4ff0-bae8-84ac989361a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\"\n",
    "\n",
    "extraction_path = \".\"\n",
    "await skillsnetwork.prepare(url = url, path = extraction_path, overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee736c1-de57-4511-82e7-96e3832e3962",
   "metadata": {},
   "source": [
    "Now, you will be able to see the **images_dataSAT** folder in the left pane. It has two folders  **class_0_non_agri** and **class_1_agri**.\n",
    "The folder structure looks as follows:\n",
    "\n",
    "```python\n",
    "images_dataSAT/\n",
    "├── class_0_non_agri/\n",
    "└── class_1_agri/\n",
    "\n",
    "```\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center;\"><b>Primary folder</b></td>\n",
    "        <td style=\"text-align:center;\"><b>Subfolders</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/owykkC4Pr2zxLtU6vskQ5A/DL0321EN-M1L1-file-tree-Screenshot-1.png\" style=\"width:300px; border:0px solid black;\"></td>\n",
    "        <td><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/okqnadJpvAeedGUXXYBIFg/DL0321EN-M1L1-file-tree-Screenshot-2.png\" style=\"width:350px; border:0px solid black;\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "**class_0_non_agri** is the non-agricultural land class, as defined earlier, and it represents images with non-cultivable land. \n",
    "\n",
    "**class_1_agri**, on the other hand, is the agricultural land class, and it represents the images with cultivable land.\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center;\"><b>class_0_non_agri</b></td>\n",
    "        <td style=\"text-align:center;\"><b>class_1_agri</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/1jSl6X5tUkVro8I_av8lEQ/DL0321EN-M1L1-file-tree-screenshot-3.png\" style=\"width:300px; border:1px solid black;\"></td>\n",
    "        <td><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9f7sT5DBeFE_6Mp2OV3JKQ/DL0321EN-M1L1-file-tree-screenshot-4.png\" style=\"width:300px; border:1px solid black;\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1526525-0f8d-4a27-8437-c516b398ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "base_dir = './images_dataSAT/'\n",
    "# Note: It's common practice for class directories to have clear names.\n",
    "# Using 'class_0' and 'class_1' is good for automatic inference.\n",
    "dir_non_agri = os.path.join(base_dir, 'class_0_non_agri')\n",
    "dir_agri = os.path.join(base_dir, 'class_1_agri')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145e7592-554a-481a-9dd6-d94bf10a833a",
   "metadata": {},
   "source": [
    "### **Task 1:** Create the list \"**all_image_paths**\" containing paths of all files for both folders, *class_0_non_agri* and *class_1_agri*, in the base directory.\n",
    "For  non_agri images, assign label \"0\" and assign label \"1\" for each image in the agri folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb41728-a67b-4269-8f57-45d5c8ac3f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to answer the question.\n",
    "# Initialize empty lists to store paths and labels\n",
    "all_image_paths = []\n",
    "all_labels = []\n",
    "\n",
    "# define the labels for each class\n",
    "label_non_agri = 0\n",
    "label_agri = 1\n",
    "\n",
    "# populate the lists\n",
    "\n",
    "for fname in os.listdir(dir_non_agri):\n",
    "    all_image_paths.append(os.path.join(dir_non_agri, fname))\n",
    "    all_labels.append(label_non_agri)\n",
    "\n",
    "for fname in os.listdir(dir_agri):\n",
    "    all_image_paths.append(os.path.join(dir_agri, fname))\n",
    "    all_labels.append(label_agri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0649a61-6012-4b86-8fff-55762c0d9a32",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\n",
    "# Initialize empty lists to store paths and labels\n",
    "all_image_paths = []\n",
    "all_labels = []\n",
    "\n",
    "# define the labels for each class\n",
    "label_non_agri = 0\n",
    "label_agri = 1\n",
    "\n",
    "# populate the lists\n",
    "\n",
    "for fname in os.listdir(dir_non_agri):\n",
    "    all_image_paths.append(os.path.join(dir_non_agri, fname))\n",
    "    all_labels.append(label_non_agri)\n",
    "\n",
    "for fname in os.listdir(dir_agri):\n",
    "    all_image_paths.append(os.path.join(dir_agri, fname))\n",
    "    all_labels.append(label_agri)\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c61c5-c8d9-428c-96a6-5baf72d0caf7",
   "metadata": {},
   "source": [
    "### **Task 2:** Create a temporary list \"**temp**\" by binding the image paths and labels using the `zip` function. \n",
    "Then, randomly select and print 5 image paths and their corresponding labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb4f56-e4f8-4464-8e9b-1359b819a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to answer the question.\n",
    "# Use zip to bind paths and labels, then shuffle this list of pairs\n",
    "temp = list(zip(all_image_paths, all_labels))\n",
    "np.random.shuffle(temp)\n",
    "all_image_paths, all_labels = zip(*temp)\n",
    "\n",
    "print(\"First 5 paths and labels (after shuffling):\", list(zip(all_image_paths[:5], all_labels[:5])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174259e-2391-40b1-96d3-402b313ecfad",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "# Use zip to bind paths and labels, then shuffle this list of pairs\n",
    "temp = list(zip(all_image_paths, all_labels))\n",
    "np.random.shuffle(temp)\n",
    "all_image_paths, all_labels = zip(*temp)\n",
    "\n",
    "print(\"First 5 paths and labels (after shuffling):\", list(zip(all_image_paths[:5], all_labels[:5])))\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fbd132-555f-4403-9cac-418291ac8a64",
   "metadata": {},
   "source": [
    "## What is custom data generator?\n",
    "\n",
    "A custom data generator function uses the `yield` keyword. Unlike a regular function that returns once and terminates, a generator can yield multiple values, pausing its state between each call. This \"lazy evaluation\" is the core principle behind memory-efficient sequential loading. For deep learning, this means we can create a generator that provides one batch of data, waits for the model to train on it, and then resumes to provide the next batch, all without loading the entire dataset into RAM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6ee8c-e940-489d-ba8a-09783b6fdf19",
   "metadata": {},
   "source": [
    "### Create the generator function\n",
    "\n",
    "The `custom_data_generator` function builds an infinite Python generator that supplies mini-batches of images and labels to a Keras model during training. It accepts four arguments: `image_paths`, a list of file locations; `labels`, the corresponding class IDs; `batch_size`, the number of samples per update; and `target_size`, a tuple dictating each image’s resize dimensions. \n",
    "\n",
    "At the start of every epoch, in the code you create an index array equal to the dataset length, **shuffle it with NumPy**, and reorder both paths and labels identically, preserving their pairing while randomizing order. The outer `while True` loop keeps the generator alive for successive epochs. Inside, a `for` loop walks through the shuffled dataset in `batch_size` steps, slicing out `batch_paths` and `batch_labels`. For each path, `tf.keras.utils.load_img` reads the image and rescales it to `target_size`; `img_to_array` converts the PIL image to a float32 NumPy array. These arrays accumulate in `batch_images`. \n",
    "\n",
    "After the inner loop finishes, the batch is transformed into a NumPy array and divided by 255.0, scaling pixel values from 0–255 to 0–1, which speeds convergence and stabilizes gradients. Finally, the generator yields a tuple (`batch_images`, `batch_labels`). It supports easy augmentation and custom preprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c5a581-ba44-426a-bf04-c66178d85a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_data_generator(image_paths, labels, batch_size, target_size=(64, 64)):\n",
    "    \"\"\"A custom data generator to feed a Keras model.\"\"\"\n",
    "    num_samples = len(image_paths)\n",
    "    while True: \n",
    "        # Shuffle data at the beginning of each epoch\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        shuffled_paths = np.array(image_paths)[indices]\n",
    "        shuffled_labels = np.array(labels)[indices]\n",
    "        \n",
    "        # Generate batch data\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_paths = shuffled_paths[offset:offset+batch_size]\n",
    "            batch_labels = shuffled_labels[offset:offset+batch_size]\n",
    "            \n",
    "            # Load and preprocess images from the batch\n",
    "            batch_images = []\n",
    "            for path in batch_paths:\n",
    "                img = tf.keras.utils.load_img(path, target_size=target_size)\n",
    "                img_array = tf.keras.utils.img_to_array(img)\n",
    "                batch_images.append(img_array)\n",
    "            \n",
    "            # Normalize and yield the batch data\n",
    "            yield np.array(batch_images) / 255.0, np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6087b68-73ee-4c4e-898a-027cb611cdc7",
   "metadata": {},
   "source": [
    "### Test and visualize the custom generator\n",
    "Before feeding a generator to a model, it is essential to test it. \n",
    "First you instantiate `custom_data_generator` using `image_paths`, `labels` and `batch_size`, deifning the key dataloading hyperparameters.\n",
    "\n",
    "Next you use `next(my_generator)`to get a batch of data. By calling `next(my_generator)`, you are asking the generator to execute its code until it hits the `yield` statement and to return the yielded values. This allows us to inspect a single batch to verify its shape, data types, and content. Visualizing the images with their corresponding labels, using tools like `matplotlib.pyplot.imshow`, provides a sanity check to ensure that the loading, preprocessing, and labeling logic is correct.\n",
    "You can display the images in two rows and 4 columns using `ax = plt.subplot(2, 4, i + 1)` code.  This ensures optimal use of resources for displaying a sample batch of images with their labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e29d5c-7bd9-4368-8373-4fef9075139f",
   "metadata": {},
   "source": [
    "### **Task 3:** Obtain a batch of data, using batch size 8 and the `custom_data_generator` function. \n",
    "Print the batch shape for the image and label the data obtained.\n",
    "Display the images and corresponding labels using `matplotlib.pyplot` library for visual inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fa12b-18d5-422d-853c-0b0a140d729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to answer the question.\n",
    "batch_size = 8\n",
    "data_generator = custom_data_generator(image_paths=all_image_paths, \n",
    "                                       labels=all_labels, \n",
    "                                       batch_size=batch_size)\n",
    " \n",
    "# Get one batch data\n",
    "images, labels = next(data_generator)\n",
    " \n",
    "# Print batch data statistics\n",
    "print(f\"Images batch shape: {images.shape}\")\n",
    "print(f\"Labels batch shape: {labels.shape}\")\n",
    " \n",
    "# Display the images in the batch\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(batch_size):\n",
    "    ax = plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(f\"Label: {int(labels[i])}\")\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33010d-4929-4e13-8f00-27bd51061858",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "batch_size = 8\n",
    "data_generator = custom_data_generator(image_paths=all_image_paths, \n",
    "                                       labels=all_labels, \n",
    "                                       batch_size=batch_size)\n",
    " \n",
    "# Get one batch data\n",
    "images, labels = next(data_generator)\n",
    " \n",
    "# Print batch data statistics\n",
    "print(f\"Images batch shape: {images.shape}\")\n",
    "print(f\"Labels batch shape: {labels.shape}\")\n",
    " \n",
    "# Display the images in the batch\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(batch_size):\n",
    "    ax = plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(f\"Label: {int(labels[i])}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06306f59-8f7d-44bc-9d77-b36bbd99e07a",
   "metadata": {},
   "source": [
    "## What is Keras' built-in utility?\n",
    "Now, instead of using a custom written data generator function, let's see how the `tf.keras.utils.image_dataset_from_directory` function works. This function is built on top of the highly efficient `tf.data` library, which is TensorFlow's native solution for creating complex and performant input pipelines. This function automatically infers class labels from a standardized directory structure (e.g., `main_dir/class_a/`, `main_dir/class_b/`) and returns a `tf.data.Dataset` object. This object is extremely memory efficient and gives the symbolic representation of a data stream, which can be manipulated and optimized for maximum performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaee94e-2931-4b5d-a0da-545c1b33baa1",
   "metadata": {},
   "source": [
    "### Create the training data \n",
    "Use the `tf.keras.utils.image_dataset_from_directory` function with image size **64 x 64** pixels and a batch size **8**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64964a86-8200-4a00-ae73-5f43a0d007c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (64, 64)\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Create a training dataset\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    base_dir,\n",
    "    labels='inferred',          # Labels are generated from directory structure\n",
    "    label_mode='int',           # Labels are encoded as integers (0, 1, ...)\n",
    "    validation_split=0.2,       # Reserve 20% of images for validation\n",
    "    subset='training',          # This is the training set\n",
    "    seed=1337,                  # Shuffle seed for reproducible splits\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc465f-d654-4449-a549-acf38285723f",
   "metadata": {},
   "source": [
    "### **Task 4:** Create validation data using batch size 8. \n",
    "The validation data is created by using the `subset` keyword in the `tf.keras.utils.image_dataset_from_directory` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833214e-ea45-4bb4-9c2f-5da29292043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to answer the question\n",
    "# Create a validation dataset\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    base_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=1337,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26380630-6262-4611-b510-973c8718e4a5",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "# Create a validation dataset\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    base_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=1337,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4681ba72-fdd6-4cf8-9edb-9df5936309d6",
   "metadata": {},
   "source": [
    "### Explore the `tf.data.Dataset` object\n",
    "The `tf.data.Dataset` object is a core component of TensorFlow's data input pipeline. It represents a sequence of elements, where each element is a tuple of (images, labels). It is designed for high-throughput, parallel processing. The `.take(1)` method is used here to fetch a single element (one batch) from this data stream for inspection. Unlike our custom generator, which loaded images as NumPy arrays, this utility loads them as TensorFlow Tensors, which is the native data type for TensorFlow operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169272f0-c9e6-4d81-bd75-c0761924a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Class names inferred by Keras: {train_ds.class_names}\")\n",
    "\n",
    "# Let's visualize one batch from the training dataset\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):  # Take one batch\n",
    "    for i in range(BATCH_SIZE):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        # Images are loaded as float32, so we convert to uint8 for display\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(train_ds.class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe79d556-783a-4910-9122-34386534c99a",
   "metadata": {},
   "source": [
    "### Configure the dataset for performance\n",
    "\n",
    "The dedicated utility, `tf.data.Dataset`, can be chained with various methods to create a highly performant pipeline. \n",
    "- **`.map(function)`:** Applies a given function to every element of the dataset. Here, it is used to apply data augmentation layers. TensorFlow can automatically parallelize this operation.\n",
    "- **`.cache()`:** Caches the dataset in memory after it's loaded from disk during the first epoch. For subsequent epochs, data will be read from the faster memory cache, significantly speeding up training. This is **only feasible if** the dataset fits in RAM.\n",
    "- **`.prefetch(buffer_size)`:** It allows the data preprocessing (done on the CPU) to happen in parallel with the model training (done on the GPU). While the GPU is busy with the current batch, the CPU is already preparing the next batch. This overlapping of tasks prevents the GPU from sitting idle and waiting for data, a phenomenon known as an I/O bottleneck.\n",
    "- **`AUTOTUNE`** lets TensorFlow dynamically decide the optimal buffer size for your computer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb77e7-aaf4-4451-bfe4-36bf059514d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "# Apply augmentation to the training dataset using map\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "\n",
    "# Configure for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(\"Dataset is now configured for high performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e445e310-ac8c-4877-8eec-9cd9a4baf88b",
   "metadata": {},
   "source": [
    "## Comparison and analysis\n",
    "\n",
    "We've now seen two ways to create a data loader in Keras. This comparison highlights the engineering trade-offs between control and convenience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796df9ca-fa3f-4e79-ab41-0cdd81e5db96",
   "metadata": {},
   "source": [
    "| Feature                  | Custom data generator                                       | In-built Keras utility (`image_dataset_from_directory`)     |\n",
    "|--------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Ease of use**          | Low. Requires significant boilerplate code for loops, shuffling, and batching. | High. A single, intuitive function call handles everything. |\n",
    "| **Code complexity**      | High. You manually manage file paths, labels, loading, and preprocessing. | Low. The function abstracts away all the complexity.          |\n",
    "| **Flexibility**          | Very high. You can implement any custom logic for loading or non-standard data structures. | Moderate. Designed for a standard directory structure (`class/images`), but very flexible within that. |\n",
    "| **Features**             | Basic. Shuffling, augmentation, and validation splits must be coded manually. | Rich. Built-in shuffling, batching, and validation splits. Seamless integration with Keras preprocessing layers. |\n",
    "| **Performance**          | Poor to moderate. A simple Python generator can become an I/O bottleneck for the GPU due to its single-threaded nature. | Excellent. Creates a `tf.data.Dataset` object, which is highly optimized and can be configured with `.cache()` and `.prefetch()` for maximum throughput. |\n",
    "| **Recommended for**      | Learning purposes or highly specialized, non-standard datasets (e.g., loading from a database or a custom binary format). | **Almost all standard image classification tasks.**         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad66021b-4c60-4b10-a486-12a21337eb70",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed ntoebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c93d34-b7eb-4be8-be50-bd1188149a25",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this lab, you built a custom data generator as an exercise to understand the principles of on-the-fly data processing. You tested the Keras in-built utilities, which are the preferred choice for real-world projects over custom-written data generators.\n",
    "\n",
    "You learned that the `tf.keras.utils.image_dataset_from_directory` function provides a simple and more performant solution. Leveraging the power of `tf.data` ensures your data pipeline can keep up with modern hardware, preventing data loading from becoming a bottleneck during model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab07c8-a31f-402d-b5d1-57b2ff9970c2",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "You are now equipped with the standard, production-ready method for loading image data in Keras!\n",
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743859d4-1222-4ca4-86a7-7365b52841e7",
   "metadata": {},
   "source": [
    "<!-- Change log\n",
    "## Change log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-06-16  | 1.0  | Aman  |  Created the lab |\n",
    "\n",
    "--!>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de0a79a-dfc4-456b-ac91-22d4d1b43d13",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd02039-32b6-4ed7-9097-834f9f0f18cb",
   "metadata": {},
   "source": [
    "\n",
    "<!-- This is a comment in HTML ## Change log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-06-11  | 1.0  | Aman  |  Created the lab |\n",
    "| 2025-06-30  | 2.0  | Gagandeeep |  ID review |\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac68671f-e9df-4816-9dc9-8cbccc524c11",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "500175c0b0c4dea7f83e80b447d82160ab781b908ca9f13f7cbabbcbb21f159f"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
